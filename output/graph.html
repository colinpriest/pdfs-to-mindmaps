<!doctype html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Paper Topics Mindâ€‘Map</title>
  <script src="https://unpkg.com/cytoscape@3.28.1/dist/cytoscape.min.js"></script>
  <style>
    html, body { height: 100%; margin: 0; font-family: system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
    #app { display: grid; grid-template-columns: 3fr 2fr; height: 100%; }
    #cy { border-right: 1px solid #eee; }
    #side { padding: 16px; overflow: auto; }
    .muted { color: #666; }
    .kv { margin: 8px 0; }
    .kv b { display: inline-block; min-width: 90px; }
  </style>
</head>
<body>
  <div id="app">
    <div id="cy"></div>
    <div id="side">
      <h2>Paper Topics Mind-Map</h2>
      <p class="muted">Click nodes to see details. Drag to rearrange. Scroll to zoom.</p>
      <div id="info"></div>
    </div>
  </div>
  <script>
    const DATA = {"elements": {"nodes": [{"data": {"id": "P6", "label": "P6", "title": "AI for better insurance - CSIRO", "type": "paper", "size": 18, "summary": "The report 'AI for Better Insurance: Enhancing Customer Outcomes amid Industry Challenges' by CSIRO and the Insurance Council of Australia highlights the critical role of artificial intelligence (AI) in transforming the insurance industry, particularly in response to rising costs and evolving risks such as climate change. It outlines how AI can enhance operational efficiency, improve risk management, and meet changing customer expectations, thereby addressing significant challenges faced by insurers. The report identifies seven key areas for advancing AI adoption, emphasizing the need for responsible governance, strategic implementation, and innovation in insurance products. This research is crucial for guiding the insurance sector towards a more resilient and customer-focused future, ultimately benefiting Australians through improved insurance outcomes.", "topics": ["Artificial Intelligence", "Insurance Industry", "Climate Change"]}}, {"data": {"id": "P2", "label": "P2", "title": "A framework for AI development transparency _ Anthropic", "type": "paper", "size": 18, "summary": "The paper emphasizes the urgent need for a targeted transparency framework in the development of Frontier AI to ensure public safety and accountability among developers. As AI technology evolves rapidly, the proposed framework aims to establish clear disclosure requirements for the largest AI systems, focusing on secure development practices while avoiding overly prescriptive regulations that could hinder innovation. By mandating public disclosure of safety practices and creating a legal framework to protect whistleblowers, the initiative seeks to enhance transparency, foster responsible AI development, and provide policymakers with the necessary information to assess potential risks and regulatory needs. This approach is crucial to balancing the transformative potential of AI with the imperative of safety and ethical responsibility.", "topics": ["AI Transparency", "Public Safety", "Regulatory Frameworks"]}}, {"data": {"id": "P8", "label": "P8", "title": "AI literature relevant for profesionalism 5658b4db-5d3c-4b17-8780-6d7ac86eca6d", "type": "paper", "size": 18, "summary": "The paper, prepared by the Professionalism workstream of the International Actuarial Association's Artificial Intelligence Task Force, serves as a draft document aimed at fostering understanding and discussion around the implications of artificial intelligence (AI) in the actuarial profession. It highlights the evolving regulatory landscape, ethical considerations, and the transformative potential of AI in the insurance sector. By compiling various resources and insights, the document emphasizes the importance of professionalism, governance, and risk management in the responsible use of AI technologies. It aims to guide actuaries in navigating the complexities of AI while ensuring adherence to ethical standards and protecting societal values.", "topics": ["Artificial Intelligence in Actuarial Practice", "Regulatory Frameworks for AI", "Ethics and Professionalism in AI"]}}, {"data": {"id": "P3", "label": "P3", "title": "Adapting LLM Ethical-Decision Methodology to Finance and Risk Management", "type": "paper", "size": 18, "summary": "This paper explores the ethical implications of using large language models (LLMs) in finance and risk management, particularly focusing on how socio-demographic modifiers can influence LLM decision-making in critical financial contexts such as loan approvals, insurance underwriting, and investment advice. By proposing ten novel research ideas that adapt previous methodologies to examine biases in LLMs, the authors highlight the potential for algorithmic discrimination in financial services, raising concerns about fairness and reliability in AI-driven decisions. The findings are crucial as they address the urgent need for ethical standards and regulatory compliance in the rapidly evolving landscape of AI applications in finance, ensuring that these technologies do not perpetuate existing biases and inequalities.", "topics": ["Ethics in AI", "Financial Services", "Algorithmic Bias"]}}, {"data": {"id": "P7", "label": "P7", "title": "AI Governance and Accountability - An Analysis of Anthropic\u2019s Claude 07bea5fc-4ae6-4469-89ad-5b6dd9887469", "type": "paper", "size": 18, "summary": "This paper explores the critical need for effective AI governance and accountability, particularly in the context of Anthropic's Claude, a foundational AI model. By analyzing Claude through established frameworks such as the NIST AI Risk Management Framework and the EU AI Act, the authors identify potential risks and propose mitigation strategies to ensure responsible AI development and deployment. The study emphasizes the importance of transparency, rigorous benchmarking, and ethical considerations in AI systems, highlighting the social impact of AI governance and the necessity for ongoing collaboration and adaptation in the evolving landscape of AI technologies.", "topics": ["AI Governance", "Accountability in AI", "Ethical AI Development"]}}, {"data": {"id": "P4", "label": "P4", "title": "Addressing bias in generative AI 1-s2.0-S0378720625000060-main", "type": "paper", "size": 18, "summary": "This paper addresses the critical issue of bias in generative AI, particularly in Large Language Models (LLMs), which have significantly impacted information management systems. The authors highlight the challenges posed by inherent biases in AI algorithms and training data, which can lead to unfair business decision-making and perpetuate societal stereotypes. By proposing a comprehensive framework that incorporates ethical considerations, policy implications, and sociotechnical perspectives, the paper aims to inspire future research and discussions on mitigating bias in LLM applications. The study serves as a call to action for information management scholars to develop interdisciplinary approaches and innovative methods to ensure fairness and transparency in AI-driven systems, ultimately enhancing their effectiveness in business practices.", "topics": ["Bias in Generative AI", "Large Language Models", "Information Management"]}}, {"data": {"id": "P13", "label": "P13", "title": "Centring Human Rights in the Governance of Artificial Intelligence 30f019c7-ea18-4b51-a3f9-ba6901ad1ed7", "type": "paper", "size": 18, "summary": "The Australian Human Rights Commission's submission to the United Nations emphasizes the critical need to center human rights in the governance of artificial intelligence (AI). It outlines various human rights risks associated with AI, including privacy concerns, algorithmic bias, and automation bias, which can exacerbate existing inequalities and lead to significant harm. The Commission advocates for binding regulatory frameworks to ensure that AI development and deployment respect human rights, suggesting that national regulation is preferable to self-regulatory models. This submission is significant as it seeks to influence global AI governance by promoting a human rights-centric approach, thereby ensuring that technological advancements do not come at the expense of fundamental rights and freedoms.", "topics": ["Human Rights and Technology", "AI Governance", "Algorithmic Bias"]}}, {"data": {"id": "P14", "label": "P14", "title": "Check Your AI_ A Framework For its Use in Actuarial Practice - Actuaries Digital - Actuaries Institute", "type": "paper", "size": 18, "summary": "The paper by Dr. Fei Huang presents a framework for integrating ethical principles into the use of artificial intelligence (AI) within actuarial practice, emphasizing the need for fairness, transparency, accountability, and privacy. As AI technologies increasingly influence decision-making in insurance and finance, the paper highlights the risks of discriminatory outcomes and the importance of ethical oversight by actuaries. By proposing an Ethical AI Lifecycle that aligns with the Actuarial Control Cycle, the framework aims to guide actuaries in embedding ethical considerations throughout the AI development process, thereby fostering public trust and ensuring that AI systems serve both efficiency and social equity.", "topics": ["AI Ethics", "Actuarial Science", "Risk Management"]}}, {"data": {"id": "P1", "label": "P1", "title": "25-00074_D61_REPORT_ICA-AIForBetterInsurance_PRINT_250818", "type": "paper", "size": 18, "summary": "The research paper 'AI for Better Insurance: Enhancing Customer Outcomes amid Industry Challenges' by CSIRO explores the transformative potential of artificial intelligence (AI) in the Australian insurance sector. It highlights the industry's current challenges, including rising costs, shifting consumer expectations, and the impact of climate change, while emphasizing the need for a consumer-centric approach to AI adoption. The paper identifies five key use cases for AI, such as automated claims processing and fraud detection, which can enhance customer experiences and operational efficiency. However, it also addresses significant risks associated with AI, including data privacy concerns and biases in decision-making, underscoring the importance of robust governance and regulatory frameworks to ensure responsible AI deployment. Overall, the paper serves as a critical reference for insurers, regulators, and policymakers aiming to navigate the complexities of AI integration in the insurance landscape.", "topics": ["Artificial Intelligence in Insurance", "Consumer Experience and Trust", "Regulatory and Governance Challenges"]}}, {"data": {"id": "P11", "label": "P11", "title": "Artificial Intelligence Governance Framework - General Actuarial Practice 298852ee-8794-483b-8b84-2a4425da931a", "type": "paper", "size": 18, "summary": "The paper presents a comprehensive governance framework for Artificial Intelligence (AI) within actuarial work, emphasizing the importance of responsible AI usage and the management of associated risks. It highlights the need for actuaries to adopt robust governance structures to mitigate biases in AI algorithms, ensure compliance with regulatory requirements, and enhance decision-making processes. By outlining key components such as roles and responsibilities, model risk management, and ongoing monitoring, the framework aims to educate actuaries on best practices for integrating AI into their work while safeguarding ethical standards and societal well-being. This document serves as a foundational resource for actuaries navigating the complexities of AI in their profession.", "topics": ["Artificial Intelligence Governance", "Risk Management", "Actuarial Practice"]}}, {"data": {"id": "P15", "label": "P15", "title": "Ethical and Data Quality Issues in the CFPB Complaints Database (2018\u20132025)", "type": "paper", "size": 18, "summary": "This paper examines the ethical and data quality issues surrounding the Consumer Financial Protection Bureau's (CFPB) Complaints Database from 2018 to 2025. It highlights significant findings related to racial disparities in financial services, unethical banking practices such as 'debanking,' and the impact of profit motives on customer service. The research underscores the importance of transparency in complaint data, which not only exposes unethical behavior but also drives accountability and competition among banks, leading to improved services for consumers, particularly marginalized groups. However, it also addresses critical data quality challenges, including inconsistencies in complaint categorization and the skewed representation of consumer experiences, which can affect the interpretation of the data. Overall, the paper emphasizes the dual role of the CFPB database as both a tool for consumer protection and a subject of scrutiny regarding its reliability and comprehensiveness.", "topics": ["Consumer Protection", "Ethical Banking Practices", "Data Quality and Consistency"]}}, {"data": {"id": "P5", "label": "P5", "title": "AI and Perception Biases in Investments - An Experimental Study aug2025", "type": "paper", "size": 18, "summary": "This paper investigates the capacity of AI systems, specifically generative AI models, to accurately reflect the diverse investment preferences of historically underrepresented investor populations. Through a large-scale experimental study involving 1,272 human respondents and 1,350 AI-generated agents, the authors reveal that default AI responses tend to overrepresent the preferences of young, high-income individuals, but this bias can be mitigated by incorporating demographic information into the AI prompts. The findings highlight the potential of AI to not only mirror human investment rationales but also to identify gaps in financial knowledge among investors. This research is significant as it addresses concerns about algorithmic bias in financial advising and suggests that AI could enhance access to investment opportunities for a broader demographic, thereby promoting inclusivity in the financial services industry.", "topics": ["Investment Preferences", "Algorithmic Bias", "Generative AI"]}}, {"data": {"id": "P16", "label": "P16", "title": "Ethical Considerations and Best Practices in LLM Development", "type": "paper", "size": 18, "summary": "This paper discusses the ethical considerations and best practices in the development of large language models (LLMs), emphasizing the importance of addressing biases inherent in AI systems. It highlights real-world implications of biased AI outputs, particularly in sensitive areas like mental health and recruitment, and advocates for measurable solutions such as differential privacy, bias-mitigation benchmarks, and continuous monitoring. By providing actionable strategies for AI professionals, the paper aims to foster the development of trustworthy LLMs that positively impact society while adhering to data protection laws and ethical guidelines.", "topics": ["Ethics in AI", "Bias Mitigation in Machine Learning", "Data Privacy and Compliance"]}}, {"data": {"id": "P10", "label": "P10", "title": "Are LLMs Enough for Hyperpartisan Fake Polarized and Harmful Content Detection - Evaluating In-Context Learning vs. Fine-Tuning 2509.07768v1", "type": "paper", "size": 18, "summary": "This paper investigates the effectiveness of Large Language Models (LLMs) in detecting hyperpartisan, fake, polarized, and harmful content across various online platforms. By comparing different adaptation paradigms, including Fine-Tuning (FT) and In-Context Learning (ICL), the study reveals that FT often outperforms ICL strategies, particularly for politically biased and fake news detection. The research spans multiple languages and datasets, highlighting the critical need for robust detection methods in an era of rampant misinformation. The findings underscore the importance of task-specific fine-tuning, even for smaller models, and provide insights into the optimal strategies for content detection, which is vital for maintaining public discourse and democratic integrity.", "topics": ["Misinformation Detection", "Natural Language Processing", "Political Bias in Media"]}}, {"data": {"id": "P17", "label": "P17", "title": "Ethical Considerations in LLM Development", "type": "paper", "size": 18, "summary": "This paper explores the ethical considerations surrounding the development of large language models (LLMs), highlighting the critical issues of bias, privacy, and accountability in AI systems. As LLMs become increasingly integrated into various sectors such as healthcare, finance, and the arts, the paper emphasizes the urgent need to address the biases that can arise from the training data and algorithms used in these models. It categorizes different types of biases\u2014such as gender, racial, cultural, socioeconomic, disability, and political biases\u2014and discusses their implications for societal inequalities and decision-making processes. The paper advocates for a comprehensive approach to mitigate these biases through technical strategies and ethical frameworks, underscoring the importance of fairness, accountability, and transparency in LLM development to ensure that these powerful tools serve all communities equitably.", "topics": ["Ethics in AI", "Bias in Machine Learning", "Accountability in AI Systems"]}}, {"data": {"id": "P9", "label": "P9", "title": "AI_PerceptionBias_aug2025", "type": "paper", "size": 18, "summary": "This paper investigates the ability of AI systems to accurately reflect the investment preferences of diverse demographic groups, particularly those historically underrepresented in investing. Through a large-scale experimental study involving both human respondents and AI-generated agents, the authors reveal that default AI responses tend to favor the preferences of young, high-income individuals, highlighting a significant algorithmic bias. However, when demographic information is incorporated into AI prompts, the responses align more closely with the diverse preferences of human investors. The findings suggest that AI can effectively capture the rationales behind investment choices and identify gaps in financial knowledge among individuals, indicating its potential to enhance financial advisory services and promote inclusivity in investment practices.", "topics": ["Investment preferences", "Algorithmic bias", "Generative AI"]}}, {"data": {"id": "P24", "label": "P24", "title": "Is AI Making it Easier or Harder to be a Professional_ - Actuaries Digital - Actuaries Institute", "type": "paper", "size": 18, "summary": "The paper discusses the implications of artificial intelligence (AI) on professionalism within the actuarial field, emphasizing the need for actuaries to navigate the ethical and practical challenges posed by AI technologies. It highlights the importance of adhering to a principles-based Code of Conduct, which includes integrity, competence, and communication, while also recognizing that the rapid evolution of AI necessitates updated guidance for professionals. The authors argue that the use of AI does not absolve actuaries of their responsibilities and that understanding potential harmful outcomes is crucial for maintaining professional standards. This paper is relevant as it addresses the intersection of technology and ethics in professional practice, urging actuaries to adapt to new tools while upholding their ethical obligations.", "topics": ["Artificial Intelligence", "Professional Ethics", "Actuarial Practice"]}}, {"data": {"id": "P21", "label": "P21", "title": "GPT-5 for Instructional Designers - by Dr Philippa Hardman", "type": "paper", "size": 18, "summary": "The paper by Dr. Philippa Hardman explores the implications of OpenAI's GPT-5 model for instructional designers, emphasizing the need for a balanced approach to harness its benefits while mitigating associated risks. It presents ten practical tests designed to help instructional designers effectively integrate GPT-5 into their workflows, ensuring accuracy, privacy, and compliance in learning materials. The insights provided are crucial for professionals in the field, as they navigate the complexities of AI-assisted instructional design, aiming to enhance learner outcomes without compromising quality or ethical standards.", "topics": ["AI in Education", "Instructional Design", "Risk Management in Technology"]}}, {"data": {"id": "P26", "label": "P26", "title": "Measuring the environmental impact of AI inference _ Google Cloud Blog", "type": "paper", "size": 18, "summary": "This paper discusses the environmental impact of AI inference, specifically focusing on the energy, emissions, and water consumption associated with Google's Gemini AI model. It highlights the importance of understanding AI's ecological footprint as its usage grows, presenting a comprehensive methodology for measuring these impacts. The findings reveal that the energy consumption per prompt is significantly lower than previous estimates, showcasing a 33x reduction in energy and a 44x reduction in carbon footprint over a year. The paper emphasizes the need for industry-wide consistency in measuring AI's resource consumption and outlines Google's commitment to improving AI efficiency while pursuing sustainable practices in data center operations. This research is crucial for responsible AI development and aims to drive progress towards more efficient AI systems.", "topics": ["Environmental Impact of AI", "AI Efficiency", "Sustainable Technology"]}}, {"data": {"id": "P18", "label": "P18", "title": "GenAI and the psychology of work PIIS1364661325000889", "type": "paper", "size": 18, "summary": "This paper explores the transformative impact of generative artificial intelligence (GenAI) on the psychology of work, emphasizing its dual role in enhancing worker productivity while also posing psychological threats to fundamental human needs such as competence, autonomy, and relatedness. As GenAI rapidly reshapes workplaces by mimicking cognitive and creative capabilities, it challenges traditional human-machine boundaries and alters the nature of work itself. The authors highlight the importance of understanding these psychological dynamics to foster human-centered workplaces that balance the benefits of GenAI with the potential risks, ultimately advocating for strategies that empower workers and mitigate adverse psychological effects.", "topics": ["Generative Artificial Intelligence", "Work Psychology", "Human-Machine Interaction"]}}, {"data": {"id": "P20", "label": "P20", "title": "Generative language models exhibit social identity biases s43588-024-00741-1", "type": "paper", "size": 18, "summary": "This paper investigates the presence of social identity biases in large language models (LLMs), revealing that these models exhibit patterns of ingroup favoritism and outgroup hostility similar to human behavior. Through a comprehensive analysis involving 77 different LLMs, the authors demonstrate that biases can be detected in both controlled experiments and real-world interactions. Importantly, the study highlights that the careful curation of training data and specialized fine-tuning can significantly reduce these biases. The findings underscore the implications for the development of equitable AI systems and the need to understand how human-LLM interactions may perpetuate existing social biases, which is crucial in addressing societal issues such as intergroup conflict and political polarization.", "topics": ["Artificial Intelligence", "Social Identity Biases", "Human-Computer Interaction"]}}, {"data": {"id": "P27", "label": "P27", "title": "Overview of Responsible AI practices for Azure OpenAI models - Azure AI services _ Microsoft Learn", "type": "paper", "size": 18, "summary": "This paper provides a comprehensive overview of responsible AI practices specifically tailored for Azure OpenAI models, highlighting the importance of addressing potential harms associated with generative AI technologies. It outlines a structured approach grounded in the Microsoft Responsible AI Standard, which includes four key stages: identifying, measuring, mitigating, and operating AI systems responsibly. By emphasizing the need for iterative testing, impact assessments, and stakeholder engagement, the paper underscores the relevance of these practices in ensuring ethical AI deployment, thereby enhancing user trust and safety in AI applications. The implications of this framework extend to developers and organizations seeking to implement AI responsibly while navigating the complexities of harmful content and privacy concerns.", "topics": ["Responsible AI", "Generative AI", "AI Risk Management"]}}, {"data": {"id": "P23", "label": "P23", "title": "Investigating the Effects of Cognitive Biases in Prompts on Large Language Model Outputs 2506.12338v1", "type": "paper", "size": 18, "summary": "This paper investigates the impact of cognitive biases, such as confirmation and availability biases, on the outputs of Large Language Models (LLMs). By systematically introducing these biases into prompts, the study reveals that even subtle biases can significantly distort LLM responses, leading to inaccuracies and unfaithful outputs. The research highlights the critical need for bias-aware prompt design and mitigation strategies, emphasizing its relevance for AI developers and users aiming to enhance the reliability of AI applications across various domains. The findings underscore the importance of understanding how cognitive biases influence human-LLM interactions, which is essential for improving the robustness of AI systems in real-world applications.", "topics": ["Cognitive Biases", "Large Language Models", "AI Ethics"]}}, {"data": {"id": "P19", "label": "P19", "title": "Generative AI and Price Discrimination in the Housing Market ssrn-4764418", "type": "paper", "size": 18, "summary": "This paper investigates the impact of generative artificial intelligence (GenAI) on price discrimination in the housing market, a critical societal issue where houses in white-dominant neighborhoods are often valued higher than similar houses in minority-dominant areas. The authors compare housing prices generated by GenAI to those generated by humans, revealing that GenAI prices are, on average, 16% lower and reduce the price discrimination gap by 15.6% when considering racial factors. This study is significant as it provides empirical evidence on how GenAI can alleviate housing price discrimination, contrasting with traditional AI models that may exacerbate biases. The findings have important implications for fair housing practices and policy-making, suggesting that GenAI could play a role in promoting equity in the housing market.", "topics": ["Generative AI", "Housing Discrimination", "Price Discrimination"]}}, {"data": {"id": "P22", "label": "P22", "title": "Guidance Resource - Artificial intelligence and discrimination in insurance pricing and underwriting 7e28af3c-e017-4146-bbc6-ba5c40e9628b", "type": "paper", "size": 18, "summary": "The Guidance Resource on artificial intelligence (AI) and discrimination in insurance pricing and underwriting, developed by the Australian Human Rights Commission in collaboration with the Actuaries Institute, addresses the critical need for clarity in applying Australia's anti-discrimination laws in the context of AI usage. As AI technologies increasingly influence decision-making in the insurance sector, the resource highlights the risks of algorithmic bias and discrimination, providing practical guidance for insurers to ensure compliance with federal laws. This initiative is significant not only for promoting fair outcomes for consumers but also for fostering responsible practices within the insurance industry amidst evolving digital landscapes and consumer expectations.", "topics": ["Artificial Intelligence", "Discrimination Law", "Insurance Pricing and Underwriting"]}}, {"data": {"id": "P12", "label": "P12", "title": "Bias and Fairness in Large Language Models - A Survey coli_a_00524", "type": "paper", "size": 18, "summary": "This paper presents a comprehensive survey of bias and fairness in large language models (LLMs), highlighting the critical issue of social bias that these models can perpetuate and amplify. The authors consolidate and formalize definitions of social bias and fairness in natural language processing, proposing intuitive taxonomies for bias evaluation metrics, datasets, and mitigation techniques. By categorizing existing literature and providing a compilation of publicly available datasets, the paper aims to empower researchers and practitioners to better understand and address the propagation of bias in LLMs. The findings underscore the importance of developing robust evaluation principles and mitigation strategies to ensure fairness in the deployment of LLMs, ultimately contributing to the responsible advancement of language technologies.", "topics": ["Bias in Natural Language Processing", "Fairness in Machine Learning", "Large Language Models"]}}, {"data": {"id": "P30", "label": "P30", "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes 2404.17218v1", "type": "paper", "size": 18, "summary": "This paper investigates the application of dual process theory to reduce social biases in large language models (LLMs) through various prompting techniques, including chain-of-thought (CoT) and System 2 prompting. By comparing the effectiveness of these methods across multiple social bias categories, the authors demonstrate that incorporating human-like personas and System 2 reasoning significantly mitigates biases in LLM outputs. The findings reveal that while CoT prompting is beneficial, it does not function identically to System 2 prompting, challenging previous assumptions in the field. This research is crucial for enhancing the ethical use of AI technologies, ensuring fairness and inclusivity in AI-driven communication and decision-making processes.", "topics": ["Social Bias in AI", "Cognitive Processes in NLP", "Prompting Techniques for LLMs"]}}, {"data": {"id": "P25", "label": "P25", "title": "Measuring and Mitigating Racial Disparities in LLMs Evidence from a Mortgage Underwriting Experiment ssrn-4812158", "type": "paper", "size": 18, "summary": "This paper investigates the racial disparities in recommendations made by large language models (LLMs) in the context of mortgage underwriting, revealing that these models tend to recommend more loan denials and higher interest rates for Black applicants compared to otherwise identical white applicants. The study highlights the significant implications of using AI in financial services, particularly regarding compliance with fair lending laws and the potential for perpetuating existing biases. By employing a systematic audit methodology, the authors demonstrate that simple prompt engineering can effectively mitigate these disparities, suggesting that while LLMs can introduce risks, they can also be guided to produce fairer outcomes. This research raises critical questions for regulators and financial institutions about the ethical deployment of AI technologies in high-stakes decision-making environments.", "topics": ["Racial Disparities in AI", "Mortgage Underwriting", "Fair Lending Practices"]}}, {"data": {"id": "P36", "label": "P36", "title": "Usage policies _ OpenAI", "type": "paper", "size": 18, "summary": "The paper outlines updated usage policies for OpenAI's services, emphasizing the importance of safe and responsible use of AI technologies. It establishes universal guidelines applicable to all users, as well as specific rules for developers utilizing the OpenAI API and ChatGPT. The policies aim to prevent harmful activities, protect user privacy, and ensure compliance with legal standards. By fostering a culture of accountability and continuous learning from real-world applications, the document highlights OpenAI's commitment to evolving its policies in response to emerging trends and challenges in AI usage, ultimately aiming to maximize innovation while safeguarding users and society.", "topics": ["AI Ethics", "Usage Policies", "User Safety"]}}, {"data": {"id": "P31", "label": "P31", "title": "Socio-Demographic Modifiers Shape Large Language Models\u2019 Ethical Decisions s41666-025-00211-x", "type": "paper", "size": 18, "summary": "This research paper investigates the ethical decision-making of large language models (LLMs) in healthcare, particularly how socio-demographic modifiers influence their responses to clinical ethical dilemmas. By analyzing the responses of nine different LLMs to 100 clinical vignettes, the study reveals that these models exhibit significant shifts in ethical choices based on socio-demographic cues, raising concerns about their alignment with established medical ethics principles such as autonomy, beneficence, and justice. The findings highlight the potential risks of bias in AI-driven healthcare applications, emphasizing the need for rigorous auditing and alignment strategies to ensure equitable care and ethical consistency in clinical decision-making processes.", "topics": ["Large Language Models (LLMs)", "Medical Ethics", "Bias in Healthcare AI"]}}, {"data": {"id": "P29", "label": "P29", "title": "Prompting Practices & Ethical Outcomes in LLMs (2018\u20132025)_ A Systematic Review", "type": "paper", "size": 18, "summary": "This systematic review examines the evolution of prompting practices in large language models (LLMs) from 2018 to 2025, highlighting their significant impact on safety, bias, and ethical outcomes. The paper identifies key findings, such as how explicit system prompts can enhance safety but may lead to over-refusals, the influence of persona framing on bias levels, and the mixed ethical outcomes of chain-of-thought prompting. It underscores the importance of prompt design in mitigating harmful outputs and improving truthfulness, while also addressing the environmental implications of prompting techniques. The findings are backed by robust evidence from multiple studies, emphasizing the need for ongoing research to refine prompting strategies and enhance the ethical deployment of AI technologies.", "topics": ["Ethics in AI", "Prompt Engineering", "Bias and Fairness in Machine Learning"]}}, {"data": {"id": "P32", "label": "P32", "title": "Stereotype Assumptions and Severity Amplification_ How Financial LLMs Discriminate When It Matters M", "type": "paper", "size": 18, "summary": "This paper conducts a thorough fairness audit of four commercial large language models (LLMs) in the context of financial complaint resolution, revealing significant discrimination patterns that violate fairness principles. The study highlights illegal disparate treatment, severe procedural bias, and a severity amplification effect, particularly affecting marginalized groups such as Black women. By employing a novel methodology that embeds subtle demographic cues within complaint narratives, the research uncovers how these models perpetuate biases even when demographic information is not explicitly stated. The findings underscore the urgent need for financial institutions to implement rigorous auditing and monitoring processes to mitigate discrimination, particularly in high-stakes scenarios, thereby ensuring compliance with regulations like the Equal Credit Opportunity Act (ECOA).", "topics": ["Fairness in AI", "Discrimination in Financial Services", "Bias in Machine Learning"]}}, {"data": {"id": "P33", "label": "P33", "title": "Systematic Review of Fairness in LLM Complaint Handling in Financial Services", "type": "paper", "size": 18, "summary": "This paper systematically reviews the impact of Large Language Models (LLMs) on the handling of consumer complaints in financial institutions from 2015 to 2025, highlighting both the transformative potential and the risks associated with their use. It emphasizes that LLMs can enhance fairness and resolution rates for consumers with limited English proficiency, but also warns of algorithmic biases that can lead to disparities in outcomes across demographic groups. The paper outlines the importance of implementing fairness metrics, governance frameworks, and regulatory compliance to mitigate these risks, ultimately advocating for a balanced approach that integrates AI's efficiency with the need for equitable treatment of all consumers. The findings underscore the necessity for financial firms to operationalize fairness checks throughout the complaint process to uphold customer trust and meet regulatory standards.", "topics": ["Artificial Intelligence in Finance", "Algorithmic Fairness", "Consumer Complaint Management"]}}, {"data": {"id": "P40", "label": "P40", "title": "Why language models hallucinate _ OpenAI", "type": "paper", "size": 18, "summary": "The paper explores the phenomenon of hallucinations in language models, where models generate confident but incorrect answers. It argues that current evaluation methods incentivize guessing over admitting uncertainty, leading to persistent hallucinations even in advanced models like GPT-5. The authors propose that improving evaluation metrics to penalize confident errors more than uncertainty could help reduce hallucinations. They emphasize that hallucinations are not an inevitable flaw but rather a consequence of how models are trained and evaluated, and they advocate for a shift in how model performance is assessed to encourage more accurate and humble responses.", "topics": ["Language Models", "Artificial Intelligence", "Evaluation Metrics"]}}, {"data": {"id": "P28", "label": "P28", "title": "Position is Power - System Prompts as a Mechanism of Bias in Large Language Models (LLMs) 2505.21091v1", "type": "paper", "size": 18, "summary": "This paper investigates the role of system prompts in Large Language Models (LLMs) as a mechanism for bias, highlighting how the placement of demographic information in these prompts can lead to significant representational and allocative biases in model outputs. The authors analyze six commercially available LLMs, revealing that system prompts, which are often opaque and layered, can disproportionately affect the representation of various demographic groups and influence decision-making processes. The findings underscore the critical need for transparency in AI systems and advocate for the integration of system prompt analysis into AI auditing practices to mitigate potential harms and ensure equitable outcomes in AI applications across diverse sectors.", "topics": ["Bias in AI", "Large Language Models", "AI Transparency"]}}, {"data": {"id": "P39", "label": "P39", "title": "which_humans_09222023", "type": "paper", "size": 18, "summary": "This paper critically examines the psychological biases inherent in Large Language Models (LLMs), particularly their tendency to reflect the perspectives of Western, Educated, Industrialized, Rich, and Democratic (WEIRD) societies. The authors argue that the training data for LLMs is disproportionately derived from WEIRD populations, leading to a skewed understanding of human psychology that overlooks the vast cultural and psychological diversity present globally. By employing comprehensive cross-cultural data, the study highlights the implications of this bias for both scientific research and ethical considerations in AI development. The findings underscore the necessity of addressing cultural diversity in the training and evaluation of LLMs to ensure that they accurately represent the broader spectrum of human experiences and values.", "topics": ["Cultural Psychology", "Artificial Intelligence", "Bias in Machine Learning"]}}, {"data": {"id": "P35", "label": "P35", "title": "Thinking Fair and Slow - On the Efficacy of Structured Prompts for Debiasing Language Models 2024.emnlp-main.13", "type": "paper", "size": 18, "summary": "This paper explores the effectiveness of structured prompting techniques for debiasing large language models (LLMs) without requiring access to their internal mechanisms. It introduces an end-user-focused iterative framework that leverages System 2 thinking processes to promote logical and reflective text generation. By evaluating various prompting strategies across multiple LLMs and datasets, the study demonstrates that more complex Implicative Prompts significantly reduce bias in outputs while maintaining competitive performance on downstream tasks. The findings highlight the potential for user-driven debiasing methods, paving the way for future research in prompt-based approaches to enhance fairness in AI-generated text.", "topics": ["Debiasing Techniques", "Large Language Models", "Natural Language Processing"]}}, {"data": {"id": "P34", "label": "P34", "title": "Systematic Review of LLM Fairness Metrics and Model Comparisons (2016\u20132025)", "type": "paper", "size": 18, "summary": "This paper explores the critical intersection of large language models (LLMs) and fairness, emphasizing the need for models to ask clarifying questions to mitigate bias in high-stakes decision-making, such as financial services. It highlights the importance of evaluating models across various decision thresholds to identify potential bias amplification and advocates for systematic benchmarking of different models to ensure equitable outcomes. The findings suggest that larger models are not inherently fairer and that governance teams must rigorously test and monitor models for bias, particularly in light of evolving regulatory expectations. This research is significant as it provides actionable recommendations for financial institutions to enhance fairness in AI applications, ensuring that models are not only accurate but also equitable.", "topics": ["Fairness in AI", "Bias in Machine Learning", "Large Language Models"]}}, {"data": {"id": "P38", "label": "P38", "title": "When AI Sets Wages Biases and Labor Discrimination in Generative Pricing ssrn-5404966", "type": "paper", "size": 18, "summary": "This paper investigates biases and discrimination in wage recommendations generated by large language models (LLMs) for online freelancers, analyzing 60,000 profiles across various categories. The study reveals that LLMs tend to recommend higher hourly rates than humans, with significant disparities based on geographic location and age, while showing no evidence of gender-based discrimination. The findings highlight the potential for LLMs to perpetuate existing biases in labor markets, emphasizing the importance of prompt design in mitigating these disparities. The implications of this research are critical for ensuring fairness in AI-mediated pricing systems and may necessitate regulatory oversight to promote equitable economic opportunities in the gig economy.", "topics": ["AI and Labor Markets", "Bias in Machine Learning", "Price Discrimination"]}}, {"data": {"id": "P41", "label": "P41", "title": "why-language-models-hallucinate", "type": "paper", "size": 18, "summary": "The paper explores the phenomenon of 'hallucinations' in large language models, where these models produce plausible yet incorrect statements instead of acknowledging uncertainty. It argues that this issue arises from the training and evaluation processes that prioritize guessing over admitting uncertainty, leading to a statistical tendency for models to generate errors. The authors analyze the causes of hallucinations through the lens of computational learning theory, demonstrating that even with error-free training data, the objectives of language model training can result in inaccuracies. The paper highlights the need for a socio-technical approach to mitigate hallucinations by modifying evaluation metrics that currently reward overconfidence, thereby steering the development of more trustworthy AI systems.", "topics": ["Language Models", "Artificial Intelligence", "Machine Learning"]}}, {"data": {"id": "P37", "label": "P37", "title": "Voice AI in Firms A Natural Field Experiment on Automated Job Interviews ssrn-5395709", "type": "paper", "size": 18, "summary": "This paper investigates the effectiveness of AI voice agents in conducting job interviews, revealing that AI can outperform human recruiters in several key metrics. Through a natural field experiment involving over 70,000 applicants, the study finds that AI-led interviews increase job offers by 12%, job starts by 18%, and 30-day retention by 17%. Despite initial skepticism from recruiters about AI performance, the results indicate that AI not only matches human capabilities but also enhances applicant satisfaction and firm operations. This research highlights the potential for AI to transform recruitment processes, suggesting that firms can leverage AI technology to improve hiring outcomes while maintaining quality and efficiency.", "topics": ["Artificial Intelligence in Recruitment", "Job Interviews", "Human-Computer Interaction"]}}, {"data": {"id": "AI_Insurance", "label": "AI in Insurance and Financial Services", "type": "topic", "size": 40, "summary": "The research topic of \"AI in Insurance and Financial Services\" explores the integration of artificial intelligence technologies within the insurance and financial sectors, focusing on their implications for decision-making, bias, and ethical considerations. The papers highlight various aspects of AI's role, including its potential to enhance efficiency and accuracy in underwriting and investment processes, while also addressing the challenges posed by biases inherent in AI systems. For instance, the paper \"AI and Perception Biases in Investments\" examines how cognitive biases can affect investment decisions, suggesting that AI could either mitigate or exacerbate these biases depending on its implementation.\n\nA significant concern within this domain is the fairness and ethical implications of AI applications, particularly in pricing and underwriting practices. The paper \"Guidance Resource - Artificial Intelligence and Discrimination in Insurance Pricing and Underwriting\" discusses how AI can inadvertently perpetuate discrimination if not carefully managed. Techniques such as bias mitigation and structured prompting are emphasized across several studies, including \"Prompting Techniques for Reducing Social Bias in LLMs\" and \"Thinking Fair and Slow,\" which propose methods to enhance the ethical outcomes of AI systems by addressing biases in large language models (LLMs). These techniques aim to refine AI outputs and ensure that they align with professional standards and ethical guidelines, ultimately fostering a more equitable landscape in financial services.\n\nOverall, the synthesis of these papers and techniques underscores the dual-edged nature of AI in insurance and finance: while it offers transformative potential for improving operational efficiencies and decision-making, it also necessitates rigorous scrutiny to prevent bias and uphold ethical standards. The ongoing research in this field aims to balance innovation with responsibility, ensuring that AI serves as a tool for better outcomes in the financial sector.", "subtopics": ["AI Voice Agents in Recruitment"]}}, {"data": {"id": "AI_Ethics_Governance", "label": "Ethics and Governance in AI", "type": "topic", "size": 40, "summary": "The research topic of Ethics and Governance in AI encompasses the frameworks and principles guiding the responsible development and deployment of artificial intelligence technologies. This area of study is increasingly critical as AI systems become more integrated into various sectors, raising concerns about transparency, accountability, and human rights. The papers listed provide a comprehensive examination of these issues from multiple perspectives, highlighting the need for robust governance structures that prioritize ethical considerations.\n\nFor instance, Anthropic's papers discuss the importance of transparency in AI development and propose frameworks for accountability, emphasizing the role of organizations in ensuring that AI systems are designed and implemented responsibly. Similarly, the \"Artificial Intelligence Governance Framework\" from the General Actuarial Practice outlines best practices for actuaries, indicating a growing recognition of the need for ethical guidelines tailored to specific professional contexts. The focus on human rights in AI governance, as explored in the paper \"Centring Human Rights in the Governance of Artificial Intelligence,\" underscores the necessity of aligning AI practices with fundamental human values.\n\nMoreover, the inclusion of practical frameworks, such as those from Microsoft and the Actuaries Institute, illustrates the application of ethical principles in real-world scenarios, ensuring that AI technologies are not only effective but also equitable and just. Overall, the synthesis of these papers highlights a collective movement towards establishing ethical standards and governance mechanisms that can guide the responsible evolution of AI, addressing both current challenges and future implications.", "subtopics": ["Artificial Intelligence Governance Framework", "Ethical Issues in Banking", "Measurement and Mitigation of AI Harms", "Non-Regulatory Measures for AI Governance"]}}, {"data": {"id": "Bias_in_AI", "label": "Bias and Fairness in AI Systems", "type": "topic", "size": 40, "summary": "The research topic of \"Bias and Fairness in AI Systems\" addresses the critical challenges posed by biases inherent in artificial intelligence, particularly in large language models (LLMs). The papers listed explore various dimensions of this issue, including the detection of hyperpartisan and harmful content, the ethical implications of LLM development, and the impact of generative AI on social equity, such as price discrimination in housing markets. For instance, the paper on hyperpartisan content detection evaluates the effectiveness of in-context learning versus fine-tuning, highlighting the need for robust methodologies to mitigate bias in AI outputs.\n\nSeveral papers focus specifically on the ethical considerations surrounding LLMs, emphasizing best practices in their development to ensure fairness and reduce social identity biases. The systematic review of fairness metrics provides a comprehensive overview of existing methodologies for assessing bias in LLMs, while studies on racial disparities in mortgage underwriting illustrate the real-world consequences of biased AI systems. Collectively, these works underscore the importance of governance frameworks and ethical guidelines in the deployment of AI technologies, aiming to foster equitable outcomes and minimize harm in diverse applications.", "subtopics": ["AI Bias and Fairness", "Bias Mitigation Techniques", "Comparison of Human and LLM Biases", "Cross-Model Fairness in Finance", "Effects of Fine-Tuning on Bias", "High-Stakes Bias Amplification", "LLM Questioning Behavior as a Fairness Metric", "Mitigation Strategies for Bias", "Political Bias Detection", "Social Identity Biases in Language Models", "Understanding LLM Bias"]}}, {"data": {"id": "AI_Impact_on_Workplace", "label": "Impact of AI on Workplaces and Employment", "type": "topic", "size": 40, "summary": "The research topic \"Impact of AI on Workplaces and Employment\" explores how artificial intelligence technologies are reshaping the dynamics of work environments and influencing employment patterns. The papers listed provide insights into specific applications of AI in workplace settings, highlighting both psychological and practical implications.\n\nThe paper \"GenAI and the psychology of work\" examines how generative AI affects employee perceptions, motivation, and overall workplace psychology. It delves into the nuances of how AI tools can alter job roles, employee engagement, and the nature of collaboration among teams. On the other hand, \"Voice AI in Firms: A Natural Field Experiment on Automated Job Interviews\" investigates the practical implementation of AI in recruitment processes, specifically through automated interviews. This study sheds light on the effectiveness and acceptance of AI-driven hiring practices, revealing potential biases and the implications for job seekers.\n\nTogether, these papers illustrate a dual perspective on the impact of AI: one focusing on the psychological ramifications for employees and the other on the operational changes in hiring practices. The integration of AI in workplaces not only transforms how tasks are performed but also raises critical questions about the future of employment, employee well-being, and the ethical considerations surrounding AI deployment in human resource management.", "subtopics": ["Impact of Generative AI on Workplaces"]}}, {"data": {"id": "AI_Environmental_Impact", "label": "Environmental Impact of AI", "type": "topic", "size": 40, "summary": "The environmental impact of artificial intelligence (AI) is an emerging area of research that examines how AI technologies contribute to ecological degradation and climate change. The paper \"Centring Human Rights in the Governance of Artificial Intelligence\" discusses the ethical implications of AI deployment, emphasizing the need for governance frameworks that prioritize human rights and environmental sustainability. This perspective highlights the interconnectedness of technological advancement and its potential consequences on both society and the environment.\n\nIn contrast, the paper \"Measuring the environmental impact of AI inference\" from Google Cloud Blog provides a more quantitative approach, focusing on the specific metrics and methodologies used to assess the carbon footprint associated with AI inference processes. This includes evaluating energy consumption during model training and deployment, which can significantly contribute to greenhouse gas emissions. Together, these papers underscore the importance of integrating ethical considerations and environmental assessments into the development and implementation of AI technologies, advocating for a balanced approach that mitigates negative impacts while harnessing the benefits of AI.", "subtopics": ["Human Rights Risks of AI"]}}, {"data": {"id": "AI_Transparency", "label": "Transparency and Accountability in AI", "type": "topic", "size": 40, "summary": "The topic of \"Transparency and Accountability in AI\" addresses the critical need for clear guidelines and frameworks that ensure artificial intelligence systems are developed and deployed responsibly. This area of research emphasizes the importance of making AI processes understandable and accountable to stakeholders, including developers, users, and the broader society. The papers listed contribute to this discourse by proposing frameworks and best practices that enhance transparency in AI development and application.\n\nFor instance, the paper from Anthropic outlines a comprehensive framework aimed at improving transparency in AI development, which is essential for fostering trust and understanding among users. Similarly, the Actuaries Institute's paper discusses the implications of AI in actuarial practice, emphasizing the need for a structured approach to ensure that AI tools are used responsibly and ethically. Microsoft's overview of responsible AI practices for Azure OpenAI models further illustrates how established tech companies are integrating transparency and accountability into their AI services, providing guidelines that can be adopted across various sectors. Collectively, these contributions highlight the ongoing efforts to create a more transparent and accountable AI landscape, ensuring that AI technologies serve the public good while minimizing risks and biases.", "subtopics": ["Fairness and Transparency in AI Modelling", "Responsible AI Practices for Azure OpenAI Models"]}}, {"data": {"id": "AI_in_Healthcare", "label": "AI Applications in Healthcare", "type": "topic", "size": 40, "summary": "The topic of AI Applications in Healthcare encompasses the integration of artificial intelligence technologies into various aspects of healthcare delivery, management, and research. This field aims to enhance patient outcomes, streamline operations, and improve decision-making processes through the use of advanced algorithms and data analytics. One significant aspect of this topic is the establishment of governance frameworks that ensure the ethical and responsible use of AI in healthcare settings.\n\nThe paper titled \"Artificial Intelligence Governance Framework - General Actuarial Practice\" highlights the importance of creating structured guidelines and policies to manage the deployment of AI technologies in healthcare. Such frameworks are essential for addressing concerns related to data privacy, algorithmic bias, and accountability, ensuring that AI applications are not only effective but also equitable and transparent. By synthesizing these governance principles with AI applications, healthcare organizations can better navigate the complexities of implementing AI solutions while safeguarding patient interests and maintaining trust in the healthcare system. Overall, the intersection of AI and healthcare presents both opportunities and challenges that necessitate careful consideration and strategic planning.", "subtopics": ["Model Validation in AI"]}}, {"data": {"id": "AI_in_Investment", "label": "AI in Investment and Financial Decision Making", "type": "topic", "size": 40, "summary": "The topic of \"AI in Investment and Financial Decision Making\" explores the transformative role of artificial intelligence in shaping how investors and financial institutions make decisions. Recent research highlights the impact of AI on various aspects of financial markets, including behavioral biases and pricing strategies. For instance, the paper titled \"AI_PerceptionBias_aug2025\" delves into how AI can mitigate perception biases among investors, enhancing decision-making processes by providing data-driven insights that counteract cognitive distortions. This suggests that AI tools can help investors make more rational choices, potentially leading to improved market efficiency.\n\nAdditionally, the paper \"Generative AI and Price Discrimination in the Housing Market\" examines the application of generative AI in real estate, particularly how it can influence pricing strategies through personalized offers based on consumer data. This research indicates that AI not only aids in investment decisions but also reshapes market dynamics by enabling more sophisticated pricing mechanisms. Together, these studies illustrate the dual role of AI in refining individual decision-making and altering broader market behaviors, underscoring its significance in the future of investment and financial strategies.", "subtopics": ["AI in Investment Analysis", "Generative AI in Housing Pricing", "Investment Preferences and Biases"]}}, {"data": {"id": "T_Unrelated", "label": "Unrelated", "type": "topic", "size": 40, "summary": "Papers with no clear topic match", "unmatched_topics": ["Evaluation of Language Models", "GPT-5 for Instructional Designers", "Language Model Hallucinations", "Monitoring for Policy Violations", "SMART/Bloom's Objective Test", "Sycophancy Reduction Test", "Uncertainty in Language Models", "Usage Policies and Safety Guidelines"]}}, {"data": {"id": "Tech:bias-mitigation-techniques", "label": "Bias Mitigation Techniques", "type": "technique", "size": 22, "summary": "Bias mitigation techniques are essential for ensuring fairness in AI systems, particularly in the insurance and financial services sectors where biased algorithms can lead to discriminatory outcomes. These techniques work by identifying biases in training data, adjusting algorithms, and monitoring results to promote equitable treatment of all customers. Their relevance is underscored by the need for compliance with fairness regulations, enhancing customer trust, and improving decision-making processes, ultimately leading to better outcomes for individuals and organizations alike.", "how_it_works": ["Bias mitigation techniques identify and analyze potential biases in datasets used for training AI models.", "These techniques can include pre-processing methods that adjust the training data to ensure a more balanced representation of different groups.", "In-processing methods modify the algorithms during training to reduce bias by applying fairness constraints.", "Post-processing techniques adjust the outcomes of AI models to ensure equitable results across different demographic groups.", "Continuous monitoring and evaluation are implemented to assess the effectiveness of bias mitigation strategies and make necessary adjustments."], "relevance": ["In the insurance and financial services sectors, biased AI models can lead to unfair treatment of customers, impacting their access to services.", "Bias mitigation techniques help ensure compliance with regulations aimed at promoting fairness and equity in lending and insurance practices.", "These techniques enhance the credibility and trustworthiness of AI systems, which is crucial for customer acceptance in financial services.", "By reducing bias, these techniques can improve decision-making processes, leading to better customer outcomes and satisfaction.", "Implementing bias mitigation strategies can help organizations avoid legal repercussions and reputational damage associated with discriminatory practices."]}}, {"data": {"id": "Tech:evaluation-metrics", "label": "Evaluation Metrics", "type": "technique", "size": 22, "summary": "Evaluation metrics are essential tools used to assess the performance of machine learning models, particularly in the context of AI applications in insurance and financial services. These metrics help quantify the accuracy, precision, recall, and overall effectiveness of predictive models, enabling organizations to make informed decisions based on data-driven insights. In the highly regulated and risk-sensitive domains of insurance and finance, robust evaluation metrics ensure that AI systems are reliable, fair, and aligned with business objectives, ultimately enhancing customer trust and operational efficiency.", "how_it_works": ["Evaluation metrics are defined based on the specific goals of the model, such as classification accuracy, precision, recall, F1 score, or area under the ROC curve.", "Once a model is trained on a dataset, it is tested on a separate validation or test dataset to evaluate its performance using the chosen metrics.", "The results from the evaluation metrics provide quantitative measures that indicate how well the model is performing in terms of making correct predictions.", "Metrics can be adjusted or weighted based on the importance of different types of errors, such as false positives versus false negatives, depending on the application.", "The evaluation process is iterative, allowing for model tuning and optimization based on the feedback from the metrics to improve performance."], "relevance": ["In insurance, evaluation metrics help assess the accuracy of risk assessment models, ensuring that premiums are set appropriately based on predicted risks.", "In financial services, these metrics are crucial for evaluating credit scoring models, helping to minimize default rates and improve lending decisions.", "Robust evaluation metrics ensure compliance with regulatory standards, which is vital in both insurance and finance sectors to maintain trust and transparency.", "They facilitate the comparison of different models, enabling organizations to select the most effective AI solutions for their specific needs.", "By using evaluation metrics, companies can better understand model limitations and biases, leading to more ethical AI practices in sensitive areas like insurance claims and financial transactions."]}}, {"data": {"id": "Tech:prompting-techniques", "label": "Prompting Techniques", "type": "technique", "size": 22, "summary": "Prompting techniques are strategies used to elicit specific responses or behaviors from AI models, particularly in natural language processing. In the context of AI in insurance and financial services, these techniques can enhance the interaction between users and AI systems, allowing for more accurate data interpretation, personalized customer service, and improved decision-making processes. By effectively designing prompts, organizations can leverage AI to streamline operations, reduce risks, and enhance customer engagement, ultimately leading to better service delivery and operational efficiency.", "how_it_works": ["Prompts are carefully crafted questions or statements designed to guide the AI's response.", "The technique involves understanding the context and desired outcome to formulate effective prompts.", "Iterative testing and refinement of prompts help in optimizing the AI's performance and accuracy.", "Feedback from AI responses is analyzed to adjust prompts for improved clarity and relevance.", "Different prompting styles (e.g., open-ended vs. closed questions) are employed based on the specific task or information needed."], "relevance": ["Prompting techniques can enhance customer interactions by providing tailored responses in insurance and financial services.", "They enable AI systems to better understand and process complex queries related to policies, claims, and financial advice.", "Effective prompting can lead to more accurate risk assessments and fraud detection in financial transactions.", "These techniques help in automating customer service, reducing response times, and improving overall customer satisfaction.", "By utilizing prompting techniques, organizations can harness AI to make data-driven decisions that align with regulatory compliance and risk management."]}}], "edges": [{"data": {"source": "P6", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P6", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P6", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P2", "target": "AI_Transparency", "rel": "member", "weight": 0.8}}, {"data": {"source": "P2", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P8", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P8", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P8", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P3", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P3", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P3", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P7", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P7", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P7", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P4", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P4", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P4", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P13", "target": "AI_Environmental_Impact", "rel": "member", "weight": 0.8}}, {"data": {"source": "P13", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P13", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P14", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P14", "target": "AI_Transparency", "rel": "member", "weight": 0.8}}, {"data": {"source": "P1", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P1", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P1", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P11", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P11", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P11", "target": "AI_in_Healthcare", "rel": "member", "weight": 0.8}}, {"data": {"source": "P15", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P5", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P5", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P5", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P16", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P10", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P17", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P9", "target": "AI_in_Investment", "rel": "member", "weight": 0.8}}, {"data": {"source": "P9", "target": "AI_in_Investment", "rel": "member", "weight": 0.8}}, {"data": {"source": "P24", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P24", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P26", "target": "AI_Environmental_Impact", "rel": "member", "weight": 0.8}}, {"data": {"source": "P18", "target": "AI_Impact_on_Workplace", "rel": "member", "weight": 0.8}}, {"data": {"source": "P20", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P20", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P20", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P27", "target": "AI_Transparency", "rel": "member", "weight": 0.8}}, {"data": {"source": "P27", "target": "AI_Ethics_Governance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P23", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P23", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P23", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P19", "target": "AI_in_Investment", "rel": "member", "weight": 0.8}}, {"data": {"source": "P19", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P22", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P12", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P12", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P12", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P30", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P30", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P30", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P25", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P31", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P31", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P31", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P29", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P29", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P29", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P32", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P32", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P32", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P33", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P33", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P33", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P40", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P28", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P28", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P28", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P39", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P39", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P35", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P35", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P35", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P34", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P34", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P34", "target": "Bias_in_AI", "rel": "member", "weight": 0.8}}, {"data": {"source": "P38", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P38", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P38", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P37", "target": "AI_Impact_on_Workplace", "rel": "member", "weight": 0.8}}, {"data": {"source": "P37", "target": "AI_Insurance", "rel": "member", "weight": 0.8}}, {"data": {"source": "P41", "target": "T_Unrelated", "rel": "member", "weight": 0.8}}, {"data": {"source": "P36", "target": "T_Unrelated", "rel": "member", "weight": 0.8}}, {"data": {"source": "P21", "target": "T_Unrelated", "rel": "member", "weight": 0.8}}, {"data": {"source": "AI_Insurance", "target": "Tech:bias-mitigation-techniques", "rel": "mentions", "weight": 1}}, {"data": {"source": "AI_Insurance", "target": "Tech:evaluation-metrics", "rel": "mentions", "weight": 1}}, {"data": {"source": "AI_Insurance", "target": "Tech:prompting-techniques", "rel": "mentions", "weight": 1}}, {"data": {"source": "AI_Insurance", "target": "AI_Ethics_Governance", "rel": "similar", "weight": 0.452}}, {"data": {"source": "AI_Insurance", "target": "Bias_in_AI", "rel": "similar", "weight": 0.432}}, {"data": {"source": "AI_Insurance", "target": "AI_Impact_on_Workplace", "rel": "similar", "weight": 0.481}}, {"data": {"source": "AI_Insurance", "target": "AI_Environmental_Impact", "rel": "similar", "weight": 0.463}}, {"data": {"source": "AI_Insurance", "target": "AI_Transparency", "rel": "similar", "weight": 0.44}}, {"data": {"source": "AI_Insurance", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.603}}, {"data": {"source": "AI_Insurance", "target": "AI_in_Investment", "rel": "similar", "weight": 0.705}}, {"data": {"source": "AI_Ethics_Governance", "target": "Bias_in_AI", "rel": "similar", "weight": 0.546}}, {"data": {"source": "AI_Ethics_Governance", "target": "AI_Impact_on_Workplace", "rel": "similar", "weight": 0.44}}, {"data": {"source": "AI_Ethics_Governance", "target": "AI_Environmental_Impact", "rel": "similar", "weight": 0.481}}, {"data": {"source": "AI_Ethics_Governance", "target": "AI_Transparency", "rel": "similar", "weight": 0.702}}, {"data": {"source": "AI_Ethics_Governance", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.471}}, {"data": {"source": "AI_Ethics_Governance", "target": "AI_in_Investment", "rel": "similar", "weight": 0.448}}, {"data": {"source": "Bias_in_AI", "target": "AI_Impact_on_Workplace", "rel": "similar", "weight": 0.433}}, {"data": {"source": "Bias_in_AI", "target": "AI_Environmental_Impact", "rel": "similar", "weight": 0.452}}, {"data": {"source": "Bias_in_AI", "target": "AI_Transparency", "rel": "similar", "weight": 0.589}}, {"data": {"source": "Bias_in_AI", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.46}}, {"data": {"source": "Bias_in_AI", "target": "AI_in_Investment", "rel": "similar", "weight": 0.453}}, {"data": {"source": "AI_Impact_on_Workplace", "target": "AI_Environmental_Impact", "rel": "similar", "weight": 0.639}}, {"data": {"source": "AI_Impact_on_Workplace", "target": "AI_Transparency", "rel": "similar", "weight": 0.413}}, {"data": {"source": "AI_Impact_on_Workplace", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.471}}, {"data": {"source": "AI_Impact_on_Workplace", "target": "AI_in_Investment", "rel": "similar", "weight": 0.473}}, {"data": {"source": "AI_Environmental_Impact", "target": "AI_Transparency", "rel": "similar", "weight": 0.497}}, {"data": {"source": "AI_Environmental_Impact", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.512}}, {"data": {"source": "AI_Environmental_Impact", "target": "AI_in_Investment", "rel": "similar", "weight": 0.49}}, {"data": {"source": "AI_Transparency", "target": "AI_in_Healthcare", "rel": "similar", "weight": 0.453}}, {"data": {"source": "AI_Transparency", "target": "AI_in_Investment", "rel": "similar", "weight": 0.436}}, {"data": {"source": "AI_in_Healthcare", "target": "AI_in_Investment", "rel": "similar", "weight": 0.543}}]}};
    const cy = cytoscape({
      container: document.getElementById('cy'),
      elements: DATA.elements,
      layout: { 
        name: 'cose', 
        animate: false,
        nodeRepulsion: 400000,
        idealEdgeLength: 100,
        nodeOverlap: 20,
        gravity: 80,
        numIter: 1000,
        initialTemp: 200,
        coolingFactor: 0.95,
        minTemp: 1.0
      },
      style: [
        { selector: 'node[type="topic"]', style: { 'background-color': '#5B8DEF', 'label': 'data(label)', 'font-size': 5, 'width': 'mapData(size, 20, 100, 15, 80)', 'height': 'mapData(size, 20, 100, 15, 80)', 'text-wrap': 'wrap', 'text-max-width': 120 }},
        { selector: 'node[type="paper"]', style: { 'background-color': '#33B679', 'label': 'data(label)', 'font-size': 4, 'width': 'mapData(size, 12, 40, 10, 30)', 'height': 'mapData(size, 12, 40, 10, 30)', 'text-wrap': 'wrap', 'text-max-width': 120 }},
        { selector: 'node[type="technique"]', style: { 'background-color': '#F6BF26', 'shape': 'round-rectangle', 'label': 'data(label)', 'font-size': 5, 'width': 'label', 'padding': '5px' }},
        { selector: 'edge[rel="member"]', style: { 'line-color': '#999', 'width': 'mapData(weight, 0.05, 0.9, 0.5, 4)', 'target-arrow-shape': 'triangle', 'target-arrow-color': '#999', 'curve-style': 'bezier' }},
        { selector: 'edge[rel="mentions"]', style: { 'line-color': '#F6BF26', 'width': 'mapData(weight, 1, 20, 0.5, 4)', 'curve-style': 'bezier' }},
        { selector: 'edge[rel="similar"]', style: { 'line-color': '#5B8DEF', 'line-style': 'dashed', 'width': 'mapData(weight, 0.25, 1.0, 1, 5)', 'curve-style': 'bezier' }}
      ]
    });
    const info = document.getElementById('info');
    function renderDetails(ele){
      const d = ele.data();
      let html = `<div class="kv"><b>ID</b> ${d.id}</div>` + `<div class="kv"><b>Type</b> ${d.type || 'edge'}</div>`;
      if (d.type === 'topic'){ 
        html += `<div class="kv"><b>Summary</b> ${d.summary || ''}</div>`; 
        if (d.subtopics && d.subtopics.length > 0) {
          html += `<div class="kv"><b>Related Concepts</b><ul>${d.subtopics.map(s => `<li>${s}</li>`).join('')}</ul></div>`;
        }
        if (d.id === 'T_Unrelated' && d.unmatched_topics && d.unmatched_topics.length > 0) {
          html += `<div class="kv"><b>Unmatched Topics</b><ul>${d.unmatched_topics.map(s => `<li>${s}</li>`).join('')}</ul></div>`;
        }
      }
      if (d.type === 'paper'){ 
        html += `<div class="kv"><b>Title</b> ${d.title}</div>`; 
        if (d.summary) { html += `<div class="kv"><b>Summary</b> ${d.summary}</div>`; }
        if (d.topics) { html += `<div class="kv"><b>Topics</b> ${d.topics.join(', ')}</div>`; }
      }
      if (d.type === 'technique'){ 
        html += `<div class="kv"><b>Technique</b> ${d.label}</div>`; 
        if (d.summary) { html += `<div class="kv"><b>Summary</b> ${d.summary}</div>`; }
        if (d.how_it_works) { html += `<div class="kv"><b>How it Works</b><ul>${d.how_it_works.map(s => `<li>${s}</li>`).join('')}</ul></div>`; }
        if (d.relevance) { html += `<div class="kv"><b>Relevance</b><ul>${d.relevance.map(s => `<li>${s}</li>`).join('')}</ul></div>`; }
      }
      if (d.rel){ html += `<div class="kv"><b>Relation</b> ${d.rel}</div>`; html += `<div class="kv"><b>Weight</b> ${d.weight || ''}</div>`; }
      info.innerHTML = html;
    }
    cy.on('tap', 'node', evt => renderDetails(evt.target));
    cy.on('tap', 'edge', evt => renderDetails(evt.target));
  </script>
</body>
</html>
